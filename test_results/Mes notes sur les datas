Notes sur les data :

---------------
About data
---------------

fr√©quence des y : 

- 0 : 1666 -> 86%
- 1 : 273 -> 14 %

1 000 000 de features

Limited Data : 1939 for training


---------------------------
Id√©e de trucs √† faire
---------------------------

id√©e 1 :


On garde en tete l'id√©e de de sur sampling ou Under sampling 

√† la place de

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(class_weight='balanced')

on fait :


from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_bal, y_bal = sm.fit_resample(X_reduced, y)

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_bal, y_bal = rus.fit_resample(X_reduced, y)

-----------------

Id√©e 2, apparement au vu de la taille du dataset et tout, lesm od√®les lin√©aires + efficaces
voici des mod√®les possibles

üîπ Logistic Regression (avec r√©gularisation L2)

üîπ Linear SVM (LinearSVC)

üîπ Random Forest (apr√®s PCA ou feature selection)

üîπ (Optionnel) LightGBM / XGBoost si tu veux booster un peu les perfs.


-----------------

Id√©e 3 :
On utilise le f1_score comme dans l'√©nonc√© pour √©valuer le mod√®le


-----------------

Id√©e 4 :
Pense √† la validation crois√©e

----------------
id√©e 5 :
Le bay√©sien que tu connais

----------------
id√©e 6 :
Les meta data

-------------------
Infos temps d'execution
-------------------

Info 1 :

Filtrer les features qui varient √† moins de 1% :
‚úÖ Features filtr√©es : (1939, 1000000) -> (1939, 477281)
‚è±Ô∏è Temps d'ex√©cution : 412.61 secondes

-------------------

Info 2 :
filtrer avec 10 000 features seulement

‚úÖ X r√©duit √† 10000 features sur 1000000 (1.00 %)
‚è±Ô∏è Temps d'ex√©cution : 0.00 secondes
‚úÖ Features filtr√©es : (1939, 10000) -> (1939, 9954)
‚è±Ô∏è Temps d'ex√©cution : 0.31 secondes
‚úÖ Dimension r√©duite : (1939, 9954) -> (1939, 50)
‚è±Ô∏è Temps d'ex√©cution : 2.06 secondes
‚úÖ Donn√©es normalis√©es : (1939, 50) -> (1939, 50)
‚è±Ô∏è Temps d'ex√©cution : 0.00 secondes
‚úÖ Pr√©traitement termin√©. Donn√©es pr√™tes pour l'entra√Ænement.
‚è±Ô∏è Temps total de traitement : 9.88 secondes

-----------------------


----------------------
Info √† propos des mod√®les
----------------------

info 1

‚úÖ Donn√©es charg√©es : X = (1939, 50), y = (1939,), meta = (1939, 9)
‚úÖ Donn√©es divis√©es :
   ‚Üí X_train : (1551, 50), y_train : (1551,)
   ‚Üí X_test  : (388, 50), y_test  : (388,)

üîπ Logistic Regression
              precision    recall  f1-score   support

           0       0.95      0.80      0.87       333
           1       0.38      0.75      0.51        55

    accuracy                           0.79       388
   macro avg       0.67      0.77      0.69       388
weighted avg       0.87      0.79      0.82       388

‚è±Ô∏è Temps √©coul√© : 0.01 secondes

üîπ Linear SVM
              precision    recall  f1-score   support

           0       0.96      0.76      0.85       333
           1       0.36      0.80      0.49        55

    accuracy                           0.77       388
   macro avg       0.66      0.78      0.67       388
weighted avg       0.87      0.77      0.80       388

‚è±Ô∏è Temps √©coul√© : 0.56 secondes

üîπ Random Forest
              precision    recall  f1-score   support

           0       0.89      1.00      0.94       333
           1       0.94      0.29      0.44        55

    accuracy                           0.90       388
   macro avg       0.92      0.64      0.69       388
weighted avg       0.90      0.90      0.87       388

‚è±Ô∏è Temps √©coul√© : 1.23 secondes
‚è±Ô∏è Temps total de traitement : 1.83 secondes

Il semblerait que Random Forest soit le meilleur, let's go faire un gris search dessus